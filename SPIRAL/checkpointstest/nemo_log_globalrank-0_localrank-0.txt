[NeMo W 2022-04-27 21:53:24 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 21:53:24 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 21:53:25 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 24
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo I 2022-04-27 21:53:25 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 21:53:25 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 21:53:25 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 21:53:25 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 21:53:25 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:53:25 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:53:25 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:53:25 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:53:25 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:53:25 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:51 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 21:54:51 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 21:54:52 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 24
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:54:52 exp_manager:492] Exp_manager is logging to ./checkpointstest, but it already exists.
[NeMo I 2022-04-27 21:54:52 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 21:54:52 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 21:54:52 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 21:54:52 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:52 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:52 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:52 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:52 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:52 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:54:52 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.
      rank_zero_deprecation(
    
[NeMo I 2022-04-27 21:55:35 collections:191] Dataset loaded with 6188992 files totalling 51305.77 hours
[NeMo I 2022-04-27 21:55:35 collections:192] 93 files were filtered totalling 0.02 hours
[NeMo I 2022-04-27 21:55:45 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 21:55:45 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 21:55:55 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 21:55:55 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo W 2022-04-27 21:57:26 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 21:57:26 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 21:57:27 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 24
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 21:57:27 exp_manager:492] Exp_manager is logging to ./checkpointstest, but it already exists.
[NeMo I 2022-04-27 21:57:27 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 21:57:27 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 21:57:27 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 21:57:27 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:57:27 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:57:27 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:57:27 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:57:27 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:57:27 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 21:57:27 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.
      rank_zero_deprecation(
    
[NeMo I 2022-04-27 21:58:10 collections:191] Dataset loaded with 6188992 files totalling 51305.77 hours
[NeMo I 2022-04-27 21:58:10 collections:192] 93 files were filtered totalling 0.02 hours
[NeMo I 2022-04-27 21:58:20 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 21:58:20 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 21:58:30 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 21:58:30 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo W 2022-04-27 22:09:04 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 22:09:04 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 22:09:05 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 24
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:09:05 exp_manager:492] Exp_manager is logging to ./checkpointstest, but it already exists.
[NeMo I 2022-04-27 22:09:05 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 22:09:05 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 22:09:05 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 22:09:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:09:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:09:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:09:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:09:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:09:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:09:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.
      rank_zero_deprecation(
    
[NeMo I 2022-04-27 22:09:48 collections:191] Dataset loaded with 6188992 files totalling 51305.77 hours
[NeMo I 2022-04-27 22:09:48 collections:192] 93 files were filtered totalling 0.02 hours
[NeMo I 2022-04-27 22:09:58 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 22:09:58 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 22:10:07 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 22:10:07 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo W 2022-04-27 22:11:04 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 22:11:04 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 22:11:05 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 24
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:05 exp_manager:492] Exp_manager is logging to ./checkpointstest, but it already exists.
[NeMo I 2022-04-27 22:11:05 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 22:11:05 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 22:11:05 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 22:11:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:05 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:53 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 22:11:53 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 22:11:54 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 24
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 24
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:11:54 exp_manager:492] Exp_manager is logging to ./checkpointstest, but it already exists.
[NeMo I 2022-04-27 22:11:54 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 22:11:54 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 22:11:54 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 22:11:54 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:54 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:54 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:54 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:54 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:54 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:11:54 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.
      rank_zero_deprecation(
    
[NeMo I 2022-04-27 22:12:38 collections:191] Dataset loaded with 6188992 files totalling 51305.77 hours
[NeMo I 2022-04-27 22:12:38 collections:192] 93 files were filtered totalling 0.02 hours
[NeMo I 2022-04-27 22:12:48 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 22:12:48 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 22:12:58 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 22:12:58 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 22:13:04 modelPT:866] Optimizer config = AdamW (
    Parameter Group 0
        amsgrad: False
        betas: [0.9, 0.98]
        eps: 1e-06
        lr: 0.003
        weight_decay: 0.01
    )
[NeMo I 2022-04-27 22:13:04 lr_scheduler:631] Scheduler "<spiral_nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fd2383439a0>" 
    will be used during training (effective maximum steps = -1) - 
    Parameters : 
    (warmup_steps: 32000
    warmup_ratio: null
    warmup_power: null
    min_lr: 0.0
    last_epoch: -1
    max_steps: -1
    )
[NeMo W 2022-04-27 22:13:05 nemo_logging:349] /home/cirrascale/frmccann/SPIRAL/SPIRAL/spiral_nemo/collections/asr/parts/wav2vec.py:126: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
      lens = (lens + 2 * conv.padding[0] - conv.dilation[0] * (conv.kernel_size[0] - 1) - 1) // conv.stride[0] + 1
    
[NeMo W 2022-04-27 22:14:16 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2022-04-27 22:14:16 spiral_pretrain:96] Application config
    name: st2vec
    model:
      train_ds:
        batch_size: 8
        drop_last: false
        shuffle: true
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_train.json,manifest_files/french_train.json,manifest_files/german_train.json,manifest_files/italian_train.json,manifest_files/spanish_train.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      validation_ds:
        batch_size: 8
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      test_ds:
        batch_size: 8
        drop_last: false
        shuffle: false
        num_workers: 12
        pin_memory: true
        manifest_dir: ~/frmccann/data/
        data_dir: ~/frmccann/data/
        manifest_filepath: manifest_files/dutch_test.json,manifest_files/french_test.json,manifest_files/german_test.json,manifest_files/italian_test.json,manifest_files/spanish_test.json
        sample_rate: 16000
        max_duration: null
        min_duration: 2.0
        crop_size: 250000
      optim:
        name: adamw
        lr: 0.003
        sched:
          warmup_steps: 32000
          warmup_ratio: null
          warmup_power: null
          name: CosineAnnealing
          min_lr: 0.0
          last_epoch: -1
          max_steps: 200000
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-06
        weight_decay: 0.01
        amsgrad: false
      loss:
        prob_ppl_weight: 0.1
        feature_loss_weight: 0.0
      quantizer:
        quantize_targets: true
        quantize_input: false
        same_quantizer: false
        targets_bottleneck_dim: null
        targets_bottleneck_act_fn: null
        targets_bottleneck_dropout: 0.0
        latent_vars: 320
        latent_groups: 2
        latent_dim: 0
        latent_temp:
        - 2
        - 0.5
        - 0.999995
      conv_feature_encoder:
        extractor_mode: default
        conv_bias: false
        conv_feature_layers:
        - - 512
          - 10
          - 5
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 3
          - 2
        - - 512
          - 2
          - 2
        - - 512
          - 2
          - 2
      transformer_encoder:
        use_pytorch_transformer: true
        dropout: 0.1
        conv:
          conv_pos: 128
          conv_pos_groups: 16
          layer_drop: 0.0
        encoder:
          encoder_layers: 12
          encoder_layerdrop: 0.05
          embedding_dim: 768
          ffn_embedding_dim: 3072
          num_attention_heads: 8
          dropout: 0.1
          attention_dropout: 0.1
          activation_dropout: 0.0
          activation_fn: gelu
          layer_norm_first: false
      masking:
        mask_prob: 0.65
        mask_type: static
        mask_emb_type: zero
        mask_other: 0
        mask_length: 10
        no_mask_overlap: false
        mask_min_space: 1
        mask_channel_prob: 0.0
        mask_channel_type: static
        mask_channel_other: 0
        mask_channel_length: 10
        no_mask_channel_overlap: false
        mask_channel_min_space: 1
        mask_shrink_to_batch_min: true
        mask_channel_shrink_to_batch_min: false
      dropout_input: 0.1
      dropout_features: 0.1
      final_dim: 0
      n_negatives: 100
      cross_sample_negatives: 0
      codebook_negatives: 0
      negatives_from_everywhere: false
      logit_temp: 0.1
      target_glu: false
      feature_grad_mult: 0.1
      expected_gpu_num: 8
    trainer:
      logger: null
      checkpoint_callback: false
      callbacks: null
      default_root_dir: null
      gradient_clip_val: 0.0
      process_position: 0
      num_nodes: 1
      gpus: 1
      auto_select_gpus: false
      tpu_cores: null
      log_gpu_memory: null
      progress_bar_refresh_rate: 50
      enable_progress_bar: true
      overfit_batches: 0.0
      track_grad_norm: -1
      check_val_every_n_epoch: 4
      fast_dev_run: false
      accumulate_grad_batches: 1
      max_epochs: 280
      min_epochs: 1
      max_steps: null
      min_steps: null
      limit_train_batches: 1.0
      limit_val_batches: 1.0
      limit_test_batches: 1.0
      val_check_interval: 1.0
      flush_logs_every_n_steps: 100
      log_every_n_steps: 50
      accelerator: ddp
      sync_batchnorm: false
      precision: 32
      weights_summary: full
      weights_save_path: null
      num_sanity_val_steps: 0
      resume_from_checkpoint: null
      profiler: null
      benchmark: false
      deterministic: false
      auto_lr_find: false
      replace_sampler_ddp: true
      detect_anomaly: false
      terminate_on_nan: false
      auto_scale_batch_size: false
      prepare_data_per_node: true
      amp_backend: native
      amp_level: null
      plugins: null
      move_metrics_to_cpu: false
      multiple_trainloader_mode: max_size_cycle
      limit_predict_batches: 1.0
      stochastic_weight_avg: false
      gradient_clip_algorithm: norm
      max_time: null
      reload_dataloaders_every_n_epochs: 0
      ipus: null
      devices: null
      strategy: null
      enable_checkpointing: true
      enable_model_summary: true
    exp_manager:
      explicit_log_dir: ./checkpointstest
      exp_dir: null
      name: st2vec
      version: null
      use_datetime_version: true
      resume_if_exists: false
      resume_past_end: false
      resume_ignore_no_checkpoint: false
      create_tensorboard_logger: true
      summary_writer_kwargs: null
      create_wandb_logger: false
      wandb_logger_kwargs: null
      create_checkpoint_callback: true
      checkpoint_callback_params:
        filepath: null
        dirpath: null
        filename: null
        monitor: val_loss
        verbose: true
        save_last: true
        save_top_k: 5
        save_weights_only: false
        mode: min
        every_n_epochs: 1
        prefix: null
        postfix: .nemo
        save_best_model: false
        always_save_nemo: false
        save_nemo_on_train_end: true
        model_parallel_size: null
      files_to_copy: null
      log_step_timing: true
      step_timing_kwargs:
        reduction: mean
        sync_cuda: false
        buffer_size: 1
    hydra:
      run:
        dir: .
      job_logging:
        root:
          handlers: null
    preprocessor: null
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:292: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:52: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=50)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.
      rank_zero_deprecation(
    
[NeMo W 2022-04-27 22:14:16 exp_manager:492] Exp_manager is logging to ./checkpointstest, but it already exists.
[NeMo I 2022-04-27 22:14:16 exp_manager:281] Experiments will be logged at checkpointstest
[NeMo I 2022-04-27 22:14:16 exp_manager:647] TensorboardLogger has been set up
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.
      rank_zero_deprecation("`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.")
    
[NeMo W 2022-04-27 22:14:16 exp_manager:881] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 4 epochs to ensure that checkpointing will not error out.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:14:16 experimental:27] Module <class 'spiral_nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2022-04-27 22:14:16 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2109: LightningDeprecationWarning: `Trainer.num_gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` instead.
      rank_zero_deprecation(
    
[NeMo I 2022-04-27 22:14:59 collections:191] Dataset loaded with 6188992 files totalling 51305.77 hours
[NeMo I 2022-04-27 22:14:59 collections:192] 93 files were filtered totalling 0.02 hours
[NeMo I 2022-04-27 22:15:09 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 22:15:09 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 22:15:19 collections:191] Dataset loaded with 1537418 files totalling 12746.27 hours
[NeMo I 2022-04-27 22:15:19 collections:192] 16 files were filtered totalling 0.00 hours
[NeMo I 2022-04-27 22:15:24 modelPT:866] Optimizer config = AdamW (
    Parameter Group 0
        amsgrad: False
        betas: [0.9, 0.98]
        eps: 1e-06
        lr: 0.003
        weight_decay: 0.01
    )
[NeMo I 2022-04-27 22:15:24 lr_scheduler:631] Scheduler "<spiral_nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fe748ed1520>" 
    will be used during training (effective maximum steps = -1) - 
    Parameters : 
    (warmup_steps: 32000
    warmup_ratio: null
    warmup_power: null
    min_lr: 0.0
    last_epoch: -1
    max_steps: -1
    )
[NeMo W 2022-04-27 22:15:26 nemo_logging:349] /home/cirrascale/frmccann/SPIRAL/SPIRAL/spiral_nemo/collections/asr/parts/wav2vec.py:126: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
      lens = (lens + 2 * conv.padding[0] - conv.dilation[0] * (conv.kernel_size[0] - 1) - 1) // conv.stride[0] + 1
    
[NeMo W 2022-04-27 22:18:28 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
      rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
    
